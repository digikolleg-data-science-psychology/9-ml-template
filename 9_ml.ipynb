{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3c62a95",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Fill in your github username and idm credentials below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84dc505",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "github_username = \"\"\n",
    "idm_credentials = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642b586a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f755b71b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **Applied Data Science in Medicine & Psychology - Assignment 9**\n",
    "## Machine Learning\n",
    "---\n",
    "**Deadline: dd.mm.yyyy, hh:mm**\n",
    "### **General Setup and Submission**\n",
    "1. Save an editable copy of this notebook to your Google Drive ``File`` $\\rightarrow$ ``Save a copy in Drive`` and remove the automatically appended \"Copy of\" text. **Do not change the original file name**.\n",
    "2. Work on the assignment in the *copied* version of the notebook. Make sure that you **replace** all parts of the tasks indicated with *YOUR CODE HERE* and **raise NotImplementedError()** (otherwise this causes an error in your implementation) or *YOUR ANSWER HERE* with your solution.\n",
    "3. If you take a break during the implementation, you can save the current status of your work in your repository following the submission instructions below. When you continue working on the assignment, do this in the version of the notebook that you saved in your Google Drive (access from your Google Colab account) or your repository.\n",
    "4. After finishing the assignment submit your assignment as follows:\n",
    "    * Check that everything runs as expected. To do that select in the menubar ``Runtime`` $\\rightarrow$ ``Restart and run all``. This will clear all your local variables and runs your notebook from the beginning.\n",
    "    * Save your notebook, click ``File`` $\\rightarrow$ ``Save``\n",
    "    * Click ``File`` $\\rightarrow$ ``Save a copy in GitHub``\n",
    "    * Select ``digikolleg-data-science-psychology/9-ml-<username>`` from the repository drop-down menu\n",
    "    * Type ``9_ml.ipynb`` into the file path text box\n",
    "    * Optionally, you may enter a description into the \"Commit message\" text box (like: \"solved task 1\" or \"ready for grading\")\n",
    "    * Make sure that the \"Include a link to Colaboratory\" box is checked\n",
    "    * Press \"Ok\"\n",
    "    * A new tab or window should open up, displaying your submitted notebook within your GitHub repository\n",
    "\n",
    "Before the assignment deadline, you can re-submit your assignment as often as you would like. This will update your assignment file in your assignment's repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feae57d8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Assignment Description**\n",
    "In this assignment, you will learn how to work with different functionalities provided by the ``scikit-learn`` library (``sklearn``) which offers various tools for predictive data analysis. We will cover four of the eight steps of the Machine Learning pipeline (described in the lecture). The assignment is divided into two parts: the main assignment and Bonus Tasks. The Bonus Tasks offer additional points that are added on top of the points earned in all other assignments, providing an opportunity to further enhance your overall score.\n",
    "\n",
    "#### **Assignment Goals**\n",
    "You are going to learn the following things in this assignment:\n",
    "1. how to apply standardization\n",
    "2. how to extract new features using dimensionality reduction\n",
    "3. how to split data into a training and test dataset\n",
    "4. how to train and evaluate a classifier\n",
    "5. how to predict values using a regression model \n",
    "6. how to assign labels using clustering algorithms\n",
    "\n",
    "#### **Assignment Point Value**\n",
    "This assignment is worth a total (up to) **19** points (5 Bonus points included). If your code passes all of the manual and automatic tests you will receive full credit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1313beb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Tasks** (14 points)\n",
    "Make sure that you now work on your *copied* version of this assignment's notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f4e322",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 0. Imports and Setup\n",
    "Please **run** the cell below to import the libraries needed for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b955034e-7820-42ac-a6a0-f10b1796cdc5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "!pip install biopsykit\n",
    "import biopsykit as bp\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import warnings\n",
    "from sklearn.datasets import load_breast_cancer, load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, mean_squared_error\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0cd249",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1. Data Loading and Preparation\n",
    "For the following tasks, we use the *Breast Cancer Wisconsin dataset* of the ``scikit-learn`` library. Have a look at the content and structure of the dataset (``dataset_cancer``):\n",
    "* What is the dataset about? \n",
    "* How many instances (also called observations) and attributes (also called features) are included? \n",
    "* How many instances are within every class?\n",
    "\n",
    "The relevant information needed for the assignment is stored within two data frames:\n",
    "* ``data_cancer`` contains all instances as rows and the attributes as columns\n",
    "* ``label_cancer`` indicates whether each breast cancer instance turned out to be malignant or benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d622e8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run cell to load the dataset\n",
    "dataset_cancer = load_breast_cancer()\n",
    "data_cancer = pd.DataFrame(dataset_cancer[\"data\"], columns=dataset_cancer[\"feature_names\"])\n",
    "label_cancer = pd.DataFrame(dataset_cancer[\"target\"], columns=[\"label\"])\n",
    "label_cancer.loc[label_cancer[\"label\"] == 0, [\"label\"]] = \"malignant\"\n",
    "label_cancer.loc[label_cancer[\"label\"] == 1, [\"label\"]] = \"benign\"\n",
    "data_cancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cec9f66-2ff7-4f89-ab9e-0100a65efa92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run cell to see the labels\n",
    "label_cancer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e6c2ec-81b2-4c36-b387-803156e39ff7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "source": [
    "#### 2. Feature Visualization\n",
    "* Visualize the features *mean radius*, *mean texture*, *mean smoothness*, *mean concavity*, and *mean symmetry* using a [seaborn.pairplot](https://seaborn.pydata.org/generated/seaborn.pairplot.html)\n",
    "* Set the plot parameters such that you can differentiate between the classes\n",
    "* Save the return values of the plot in a variable called ``pp``\n",
    "* Have a look at the plot. Describe your thoughts about (in the cell below):\n",
    "    * the distributions of the single features (diagonal). Why is it most of the time advantageous to include more than one feature in your analysis? \n",
    "    * the separability of classes\n",
    "    * correlation between features\n",
    "\n",
    "**Hint**: Concatenating all relevant data to a new data frame might help you to solve this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b516545a-53ee-42b0-867c-8115d1b273fd",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4291e7b2-c71c-49ac-bb3b-28f811894dce",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "051af6edc1f876efa136db82d31c141e",
     "grade": false,
     "grade_id": "pairplot_1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abd3a2c-e347-4e31-b0a7-74c5b4976f25",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79bdcc8ec7964642fead5a960e6f03f8",
     "grade": true,
     "grade_id": "pairplot_1_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run cell to test your implementation\n",
    "assert type(pp) == sns.axisgrid.PairGrid, \"Check that you create a pairplot using the seaborn library\"\n",
    "assert pp.x_vars == pp.y_vars and np.shape(pp.axes) == (5, 5), \"Check that you create a pairplot\"\n",
    "assert set(pp.x_vars) == {\"mean concavity\", \"mean radius\", \"mean smoothness\", \"mean symmetry\", \"mean texture\"}, \"Check the features in the pairplot\"\n",
    "assert pp.legend is not None, \"Make sure that you can differentiate between the classes\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4478a19-d3a1-4f3c-8bd4-763993e6a315",
   "metadata": {},
   "source": [
    "#### 3. Standardization (step: Data Preparation)\n",
    "Create two functions ``train_scaler`` and ``apply_scaler`` that apply [Standardization](https://scikit-learn.org/stable/modules/preprocessing.html) to the dataset:\n",
    "* ``train_scaler`` has the parameter ``data`` and fits a Standard Scaler to ``data``:\n",
    "    * ``data``: ``pandas.DataFrame`` containing the dataset (rows: observations, columns: features)\n",
    "    * Create an instance of [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "    * Use its methods to fit the scaler to ``data``\n",
    "    * The function should return the trained scaler object\n",
    "* ``apply_scaler`` has the parameter ``scaler`` and ``data``:\n",
    "    * ``scaler``: trained instance of  [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "    * ``data``: ``pandas.DataFrame`` containing the dataset (rows: observations, columns: features)\n",
    "    * Use the methods of ``scaler`` to transform ``data`` such that every feature individually has zero mean and unit variance\n",
    "    * The function should return an array containing a standardized version of ``data``\n",
    "* Why do you think we have to split the fitting and transforming procedure into two separate functions? (answer in the cell below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa8a3d9-81fd-47fe-bfca-a2c4376bc8aa",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfbbc84-b199-44be-9b06-4603b23289de",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4badd957409abfcdeced194efcef9e3",
     "grade": false,
     "grade_id": "standardization",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7c5dad-c4c7-444e-a787-e6077cc3ec70",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9623b50071b0cb3b7430ae3afbdd075",
     "grade": true,
     "grade_id": "standardization_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run cell to test your implementation\n",
    "scaler = train_scaler(data_cancer)\n",
    "assert type(scaler) == sklearn.preprocessing._data.StandardScaler, \"Check that you train a Standard Scaler\"\n",
    "assert hasattr(scaler, \"mean_\"), \"Ensure that you fit the Scaler to your data\"\n",
    "data_scaled = apply_scaler(scaler, data_cancer)\n",
    "assert type(data_scaled) == np.ndarray, \"Check that you return the standardized data as array\"\n",
    "assert np.shape(data_scaled) == (569, 30), \"Check that you return value keeps the input dimensions\"\n",
    "assert np.sum(np.std(data_scaled, axis=0)) > 29.99, \"Ensure that all features have unit variance after standardizing\"\n",
    "assert np.abs(np.sum(np.mean(data_scaled, axis=0))) < 1e-10, \"Ensure that all features have zero mean after standardizing\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b02a2-1454-4f19-853c-e1d929544cab",
   "metadata": {},
   "source": [
    "#### 4. Visualization Standardization\n",
    "Create a figure with two subplots that visualizes 2 features of the original and standardized dataset:\n",
    "* Create a figure with two subplots using [matplotlib.pyplot.subplots](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html) and save its return values in variables called ``fig`` and ``axs``\n",
    "* Plot in the first subplot a scatter plot of *mean radius* and *mean texture* of the original dataset\n",
    "* Plot in the second subplot a scatter plot of *mean radius* and *mean texture* of the standardized dataset\n",
    "* Set x- and y-axis labels and the subplot titles according to the data\n",
    "* Describe the effects that standardization has on the data (in the cell below)\n",
    "\n",
    "**Hints**: Check the data type of the standardized data, how can you access its values? You can also set the parameters ``sharex`` and ``sharey`` of [matplotlib.pyplot.subplots](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html) to ``True`` to see some of the effects of standardization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c50079-c512-4681-86db-a835c82fec03",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fc6b98-acbc-4f46-9bfb-d279c936e510",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf36a317fbe79ebdf5fde6c3e23d5231",
     "grade": false,
     "grade_id": "vis_standardization",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bec6a3a-7581-4392-b217-066c49ad231c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31f53c7de351240f2aa9a8f01bf3ce4f",
     "grade": true,
     "grade_id": "vis_standardization_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run cell to test your implementation\n",
    "assert len(fig.axes) == 2, \"Ensure that you create a figure with two subplots\"\n",
    "assert np.all(np.abs(fig.axes[1].xaxis.get_data_interval()) < 5) and np.all(np.abs(fig.axes[1].yaxis.get_data_interval()) < 5), \"Ensure that you visualize the standardized versions of mean radius and mean texture in the right subplot\"\n",
    "assert len(fig.axes[0].get_xlabel()) != 0 and len(fig.axes[0].get_ylabel()) != 0, \"Check the first subplot's x and y labels\"\n",
    "assert len(fig.axes[0].title.get_text()), \"Check the first subplot's title\"\n",
    "assert len(fig.axes[1].get_xlabel()) != 0 and len(fig.axes[1].get_ylabel()) != 0, \"Check the second subplot's x and y labels\"\n",
    "assert len(fig.axes[1].title.get_text()), \"Check the second subplot's title\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9faa99-de21-43b1-829f-b34ca29c64db",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 5. Feature Visualization\n",
    "* Visualize the features *mean radius*, *mean perimeter*, *mean area*, *worst radius*, and *worst perimeter* of the original dataset using a [seaborn.pairplot](https://seaborn.pydata.org/generated/seaborn.pairplot.html)\n",
    "* Set the parameters such that you can differentiate between the classes\n",
    "* Save the return values of the plot in a variable called ``pp``\n",
    "* Have a look at the plot. Describe your thoughts about the correlation between features (in the cell below):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a06cb3-5cb2-4956-b2aa-a882616e876f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ae638f-2919-48b7-99a8-9f2dae7b2e51",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4caf0aa33c73c727d30ad7e2e943086d",
     "grade": false,
     "grade_id": "pairplot_2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7cabf4-db3d-4652-9d49-2c7401f70964",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94ca489dd28aad4afb679313e126c03b",
     "grade": true,
     "grade_id": "pairplot_2_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run cell to test your implementation\n",
    "assert type(pp) == sns.axisgrid.PairGrid, \"Check that you create a pairplot using the seaborn library\"\n",
    "assert pp.x_vars == pp.y_vars and np.shape(pp.axes) == (5, 5), \"Check that you create a pairplot\"\n",
    "assert set(pp.x_vars) == {\"mean area\", \"mean perimeter\", \"mean radius\", \"worst perimeter\", \"worst radius\"}, \"Check the features in the pairplot\"\n",
    "assert pp.legend is not None, \"Make sure that you can differentiate between the classes\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c7eb84",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 6. Dimensionality Reduction (step: Data Preparation)\n",
    "Create two functions ``train_pca`` and ``apply_pca`` that apply [Principal Component Analysis (PCA)](https://scikit-learn.org/stable/modules/decomposition.html#decompositions) to the dataset:\n",
    "* ``train_pca`` has the parameters ``data`` and ``n_components`` and fits a PCA to ``data``:\n",
    "    * ``data``: ``numpy.ndarray`` or ``pandas.DataFrame`` containing the data (rows: observations, columns: features)\n",
    "    * ``n_components``: number of principal components to be kept\n",
    "    * Create an instance of [sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). Set the parameters: *n_components*: ``n_components``, *svd_solver*: \"full\". Have a look at the *n_components* parameter, which options do you have to specify the parameter? \n",
    "    * Use its methods to fit the PCA to ``data`` \n",
    "    * The function should return the trained PCA object\n",
    "* ``apply_pca`` has the parameters ``pca`` and ``data`` and transforms ``data`` using ``pca``:\n",
    "    * ``pca``: trained instance of [sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "    * ``data``: ``numpy.ndarray`` or ``pandas.DataFrame`` containing the data (rows: observations, columns: features)\n",
    "    * Use the methods of ``pca`` to transform ``data``\n",
    "    * The function should return an array containing the principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027b6563",
   "metadata": {
    "deletable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c9906d51c27b217c214473714f68026",
     "grade": false,
     "grade_id": "pca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e83db75-f9bb-4c9c-8bf7-c989fc949471",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e178e3a9656acc38b351eb56c6514e0",
     "grade": true,
     "grade_id": "pca_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run cell to test your implementation\n",
    "pca = train_pca(data_cancer, 2)\n",
    "assert type(pca) == sklearn.decomposition._pca.PCA, \"Check that you train an return a PCA\"\n",
    "assert hasattr(pca, \"components_\"), \"Ensure that you fit the PCA to your data\"\n",
    "data_pca = apply_pca(pca, data_cancer)\n",
    "assert np.shape(data_pca) == (569, 2), \"Check that you return the transformed data\"\n",
    "assert np.all(np.abs(np.mean(data_pca, axis=0)) < 1e-8), \"Check that you return the transformed data\"\n",
    "assert np.all(np.mean(data_pca, axis=0) != data_cancer.mean(axis=0)[0:2].to_numpy()), \"Check that you return the transformed data\"\n",
    "pca = train_pca(data_cancer, 0.8)\n",
    "data_pca = apply_pca(pca, data_cancer)\n",
    "assert np.shape(data_pca) == (569, 1), \"Check that you accept also a value for explained variance as n_components\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc01528f-b571-4fbb-85f8-c0fb114a4d1d",
   "metadata": {},
   "source": [
    "#### 7. PCA Visualization\n",
    "Use the functions of the tasks before to create two versions of the data set:\n",
    "* ``data_pca``: consists of two principal components of the original data set\n",
    "* ``data_scaled_pca``: consists of two principal components of the standardized data set\n",
    "\n",
    "Run the cell below the tests to visualize the PCA results of the original and standardized data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3624cdd3-d9ab-4205-9f5b-461b8ed0cf5f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a05e555c1d064c61924bc094decbf493",
     "grade": false,
     "grade_id": "vis_pca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ed65c-6617-4a5d-bcac-0e2df7c75d5a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "214bbefa00efd11b46cbffbdefa6489f",
     "grade": true,
     "grade_id": "vis_pca_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run cell to test your implementation\n",
    "assert np.shape(data_pca) == (569, 2),  \"Check that you reduce the original dataset to two principal components\"\n",
    "assert np.std(data_pca) > 470, \"Check that you apply pca to the original dataset in data_pca\"\n",
    "assert np.shape(data_scaled_pca) == (569, 2), \"Check that you reduce the standardized dataset to two principal components\"\n",
    "assert np.std(data_scaled_pca) > 3, \"Check that you standardized the data in data_scaled_pca before applying pca\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bde7c7-3a23-43f9-a9c1-076dfa47c5d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run cell to visualize the PCA results of the original and standardized dataset \n",
    "fig, axs = plt.subplots(ncols=2, figsize=(8,4))\n",
    "\n",
    "axs[0].scatter(data_pca[label_cancer[\"label\"]==\"malignant\", 0], data_pca[label_cancer[\"label\"]==\"malignant\", 1], alpha=0.5, label=\"malignant\")\n",
    "axs[0].scatter(data_pca[label_cancer[\"label\"]==\"benign\", 0], data_pca[label_cancer[\"label\"]==\"benign\", 1], alpha=0.1, label=\"benign\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].scatter(data_scaled_pca[label_cancer[\"label\"]==\"malignant\", 0], data_scaled_pca[label_cancer[\"label\"]==\"malignant\", 1], alpha=0.5, label=\"malignant\")\n",
    "axs[1].scatter(data_scaled_pca[label_cancer[\"label\"]==\"benign\", 0], data_scaled_pca[label_cancer[\"label\"]==\"benign\", 1], alpha=0.1, label=\"benign\")\n",
    "axs[1].legend()\n",
    "\n",
    "for ax, t in zip(axs, [\"PCA on original dataset\", \"PCA on standardized dataset\"]):\n",
    "    ax.set_xlabel(\"principal component 1\")\n",
    "    ax.set_ylabel(\"principal component 2\")\n",
    "    ax.set_title(t)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68830bc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 8. Train Test Split (step: Data Segregation)\n",
    "Create a function ``split_data`` that takes the parameters ``data``, ``labels``, and ``test_size`` and splits ``data`` and ``labels`` into a training and test dataset:\n",
    "* ``data``: ``numpy.ndarray`` or ``pandas.DataFrame`` containing the dataset (rows: observations, columns: features)\n",
    "* ``labels``: ``numpy.ndarray`` or ``pandas.DataFrame`` containing the labels for all observations\n",
    "* ``test_size``: ``float`` between 0 and 1, describing the size of the test dataset\n",
    "* Use the [sklearn.model_selection.train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to generate a train test split from ``data`` and ``labels``\n",
    "    * Use the *stratify* parameter of the function\n",
    "    * Set the random state to 10\n",
    "* The function should return a tuple of the into train and test dataset split dataset (in this order: data_train, data_test, label_train, label_test)\n",
    "* Describe your thoughts about the following questions in the cell below:\n",
    "    * Why do you think a Train Test Split is necessary for classifier training?\n",
    "    * In which cases could stratification be important?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df73cbd-ae31-4cd0-a015-81e1dc37babc",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b6700f",
   "metadata": {
    "deletable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70e89c1db1dc440ca5c0ff8ace5f1b1d",
     "grade": false,
     "grade_id": "train_test",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a3bc09",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f881e9ad0c9439f2af596bf3cff5263",
     "grade": true,
     "grade_id": "train_test_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run cell to test your implementation\n",
    "data_test = np.array(range(100)).reshape(20, -1); label_test = np.zeros(20); label_test[2:] = 1\n",
    "data_split = split_data(data_test, label_test, 0.3)\n",
    "assert type(data_split) == tuple, \"Check that you return a tuple of your return values\"\n",
    "assert np.shape(data_split[0]) == (14, 5) and np.shape(data_split[1]) == (6, 5) and np.shape(data_split[2]) == (14,) and np.shape(data_split[3]) == (6,), \"Check that your return values are in the right order\"\n",
    "assert np.sum(data_split[3]) < 6, \"Check that you set the stratify parameter correctly\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85b3a50-cab9-4b92-ad02-74a9582cb407",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "source": [
    "#### 9. Classification\n",
    "In this task, you will create functions that you will later need to train a classifier and evaluate its performance. \n",
    "##### 9.1 Classifier Training (step: Model Training)\n",
    "Create a function ``train_classifier`` with the parameters ``data`` and ``labels`` that fits a Support Vector Machine (SVM) to ``data``:\n",
    "* ``data``: ``numpy.ndarray`` or ``pandas.DataFrame`` containing the dataset (rows: observations, columns: features)\n",
    "* ``labels``: ``numpy.ndarray`` or ``pandas.DataFrame`` containing the labels for all observations\n",
    "* Create an instance of [sklearn.svm.SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html). Set the parameters *kernel*: \"rbf\", *random_state*: 10.\n",
    "* Use its methods to fit the SVM to ``data`` and ``labels``\n",
    "* The function should return the trained classifier object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4ab7e6-a078-436c-8323-716f101abd64",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61b6a92fb287355eb18c79fec3895b27",
     "grade": false,
     "grade_id": "train_classifier",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fabbbbb-adc6-4eaf-ac86-c6262d8e602b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5b8718170bbd01367ee968da8b82110",
     "grade": true,
     "grade_id": "train_classifier_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run cell to test your implementation\n",
    "data_test = np.array(range(50)).reshape(10, -1); label_test = np.zeros(10); label_test[5:] = 1\n",
    "clf = train_classifier(data_test, label_test)\n",
    "assert type(clf) == sklearn.svm._classes.SVC, \"Ensure that you train a classifier of type sklearn.svm.SVC\"\n",
    "assert clf.kernel == \"rbf\", \"Ensure that you set your kernel to rbf\"\n",
    "assert clf.random_state == 10, \"Ensure that you set the classifier's random_state to 10\"\n",
    "assert hasattr(clf, \"classes_\"), \"Ensure that you train the classifier\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23933fef-751d-427e-9ff2-0f6a6e8d9c3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 9.2 Classifier Evaluation - Mean Accuracy (step: Model Evaluation)\n",
    "Create a function ``calc_score`` with the parameters ``clf``, ``data``, and ``labels``\n",
    "* ``clf``: trained classifier object\n",
    "* ``data``: ``numpy.ndarray`` or ``pandas.DataFrame`` containing the dataset (rows: observations, columns: features)\n",
    "* ``labels``: ``numpy.ndarray`` or ``pandas.DataFrame`` containing the labels for all observations\n",
    "* Use the methods of ``clf`` to calculate the classifier's mean accuracy using ``data`` and ``labels``\n",
    "* The function should return the mean accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad13e2e-b02a-4a2b-b56a-28b7ba71c93a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "320ecb373d601f84c6f22d5d0c39bc6f",
     "grade": false,
     "grade_id": "calc_score",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee0ac13-4615-45ef-b6d4-af34553120a8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "59147522730fde8d247d91ec3b98e402",
     "grade": true,
     "grade_id": "calc_score_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run cell to test your implementation\n",
    "data_test = np.array(range(50)).reshape(10, -1); label_test = np.zeros(10); label_test[5:] = 1\n",
    "clf = train_classifier(data_test, label_test)\n",
    "data_test = np.array(range(10, 60)).reshape(10, -1)\n",
    "assert calc_score(clf, data_test, label_test) == 0.8, \"Ensure that you calculate the accuracy using the trained classifier and the parameters data and labels\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a64effd-2542-424f-bc29-f465927561af",
   "metadata": {},
   "source": [
    "##### 9.3 Classifier Evaluation - Confusion Matrix (step: Model Evaluation)\n",
    "Create a function ``plot_confusion_matrix`` with the parameters ``clf``, ``data``, and ``labels`` that plots a confusion matrix according to the data:\n",
    "* ``clf``: trained classifier object\n",
    "* ``data``: ``numpy.ndarray`` or ``pandas.DataFrame`` containing the dataset (rows: observations, columns: features)\n",
    "* ``labels``: ``numpy.ndarray`` or ``pandas.DataFrame`` containing the labels for all observations\n",
    "* Use the example code of [sklearn.metrics.ConfusionMatrixDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) to create a plot of a confusion matrix based on ``data`` and ``labels``\n",
    "* The function should plot the confusion matrix and return the [sklearn.metrics.ConfusionMatrixDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) object (just for testing purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6def5ba6-6e2d-41d4-93e3-a8ff3a95ed19",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79919e7ea6eabb815254d62bd9411bf0",
     "grade": false,
     "grade_id": "plot_cm",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c047357b-388f-4ed7-96e0-1da7fd22a001",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5c9a001b066a7d83d938e927802781e",
     "grade": true,
     "grade_id": "plot_cm_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run cell to test your implementation\n",
    "data_test = np.array(range(50)).reshape(10, -1); label_test = np.zeros(10); label_test[5:] = 1\n",
    "clf = train_classifier(data_test, label_test)\n",
    "cm = plot_confusion_matrix(clf, data_test, label_test)\n",
    "assert type(cm) == sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay\n",
    "assert np.all(cm.confusion_matrix == np.array([[5, 0], [0, 5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1408eb68-b371-45d0-a06d-492f03325aed",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "source": [
    "##### 9.4 Train a SVM classifier on a training data set and evaluate its performance on a separate test data set. To do so:\n",
    "* Create a train test split of your data with a test size of 0.3\n",
    "* Standardize the training data set and reduce its dimensionality to two components using PCA\n",
    "* Train the classifier and save it in a variable called ``clf``\n",
    "* Evaluate the performance of the trained classifier with the test data set:\n",
    "    * Which steps do you have to perform on the independent test data set before?\n",
    "    * Plot the confusion matrix for the test data set\n",
    "    * Calculate the mean accuracy for the train and test data set separately. Store the values in variables called ``clf_train_acc`` and ``clf_test_acc``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a143a4c-c5a0-4413-bef7-694aba4e81ad",
   "metadata": {
    "deletable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5dd0e3aeaa4639a44cd19214cb8c0b4d",
     "grade": false,
     "grade_id": "train_eval_classifier",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2759c73c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5782b8af4ffbe35bc97df89c723e61b7",
     "grade": true,
     "grade_id": "train_eval_classifier_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run cell to test your implementation\n",
    "assert clf_train_acc > 0.94 and clf_test_acc > 0.93, \"Check that you perform all the steps you performed for the training data set also for the test data set\"\n",
    "assert clf.predict([[0, 5]])[0] == \"benign\" and clf.predict([[5, 0]])[0] == \"malignant\", \"Check that you performed the Standardization and PCA on the training data set before training the classifier\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b18014-7e54-41d8-91aa-d0869a3fa8f0",
   "metadata": {},
   "source": [
    "##### 9.5 Use the given function ``plot_decision_boundary()`` to visualize the decision boundary for your classifier:\n",
    "* ``clf``: trained classifier\n",
    "* ``data``: ``numpy.ndarray`` containing the training data set\n",
    "* ``label``: ``pandas.DataFrame`` containing the labels for the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a3849-126c-4693-970a-0d5596bd30ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_decision_boundary(clf, data, label):\n",
    "    x, y = np.meshgrid(np.linspace(data[:, 0].min(), data[:, 0].max()), np.linspace(data[:, 1].min(), data[:, 1].max()))\n",
    "    grid = np.vstack([x.ravel(), y.ravel()]).T\n",
    "\n",
    "    y_pred = np.reshape(clf.predict(grid), x.shape)\n",
    "    y_pred[y_pred==\"malignant\"] = 0\n",
    "    y_pred[y_pred==\"benign\"] = 1\n",
    "    y_pred = np.array(y_pred, dtype=float)\n",
    "\n",
    "    display = DecisionBoundaryDisplay(xx0=x, xx1=y, response=y_pred)\n",
    "    display.plot()\n",
    "    display.ax_.scatter(data[label[\"label\"]==\"malignant\", 0], data[label[\"label\"]==\"malignant\", 1], label=\"malignant\", alpha=0.5)\n",
    "    display.ax_.scatter(data[label[\"label\"]==\"benign\", 0], data[label[\"label\"]==\"benign\", 1], label=\"benign\", alpha=0.5)\n",
    "    display.ax_.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ebd993-33b4-4ed4-8442-5e525fe5edae",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb2ac897179220b0b35ef7e7954b27de",
     "grade": false,
     "grade_id": "vis_decision_boundary",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceacdb4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Bonus Tasks** (5 points)\n",
    "The Bonus Tasks consist of three sub-tasks about Regression, Clustering, and Classification.\n",
    "#### 1. Regression\n",
    "In this task, we use the Diabetes dataset provided by ``sklearn``. The dataset includes four baseline features like age and body mass index and six blood serum measurements of 442 diabetes patients. All 10 features are standardized. As labels the response of interest, a quantitative measure of disease progression one year after baseline is provided. For more information, you can read the description provided by ``sklearn`` (``diabetes[\"DESCR\"]``). The goal of this task is to predict the disease progression from the features using a regression model. For easier visualization, we will perform the regression only on one of the provided features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f6a2e6-2243-4c8b-a9a4-88e813f57333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run cell to load the diabetes features and label data frames\n",
    "diabetes = load_diabetes()\n",
    "data_diabetes = pd.DataFrame(data=diabetes[\"data\"], columns=diabetes[\"feature_names\"])\n",
    "label_diabetes = pd.DataFrame(diabetes[\"target\"], columns=[\"label\"])\n",
    "data_diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae08539a-6584-4aab-b0a4-34cc9bc2fee6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f26a7d-56a2-4152-8a63-70cad9d53c0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 1.1 Since we do not have distinct class values as labels, we cannot use the ``split_data()`` function from above to divide our data into a train and test dataset. Therefore, we will use the last 130 observations (approximately 30 %) as test dataset. Create four data frames called ``X_train``, ``X_test``, ``Y_train``, and ``Y_test``:\n",
    "* ``X_Train`` should contain the first 312 elements of column **s5** of ``data_diabetes``\n",
    "* ``X_Test`` should contain the last 130 elements of column **s5** of ``data_diabetes``\n",
    "* ``Y_Train`` should contain the first 312 labels of ``label_diabetes``\n",
    "* ``Y_Test`` should contain the last 130 labels of ``label_diabetes``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24126a08-d943-47c6-ab77-2eb8da8c70f0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3350b683eace0e624aae3a7e2cb5f135",
     "grade": false,
     "grade_id": "regr_data_split",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_size = 130 # observations\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53748baa-eaaa-4220-aa20-e8c4594e2567",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f14406e8d2421c87a45fb5bf6947331",
     "grade": true,
     "grade_id": "regr_data_split_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run cell to test your implementation\n",
    "assert type(X_train) == pd.DataFrame and type(X_test) == pd.DataFrame and type(Y_train) == pd.DataFrame and type(Y_test) == pd.DataFrame, \"Check that all four variables are of type pandas.DataFrame\"\n",
    "assert X_train.shape == (312, 1) and Y_train.shape == (312, 1) and X_test.shape == (130, 1) and Y_test.shape == (130, 1), \"Check that you make the split correctly. The test set must have 130 observations\"\n",
    "assert X_train.sum()[0] > -0.8 and X_train.sum()[0] < -0.7, \"Check that you use the last 130 observations as test set\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc46274-7cf9-444e-b97b-0f1210d0606e",
   "metadata": {},
   "source": [
    "##### 1.2 Fit a Linear Regression to ``X_train`` and ``Y_train`` and evaluate the performance on ``X_test`` and ``Y_test``:\n",
    "* Create an instance of [sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) called ``regr`` and use its methods to fit the model to ``X_train`` and ``Y_train``\n",
    "* Calculate the Mean Squared Error between the true and predicted labels. You can use [sklearn.metric.mean_squared_error()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) for that. Save the value in a variable called ``test_error``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1568053e-086b-4cb1-a5a6-76c4d984df88",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3fa4986bf9e638f58be4a5c1a5a446a",
     "grade": false,
     "grade_id": "train_regression",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(\"Mean squared error: %.2f\" % test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880ec236-a880-4c4c-8b14-3a732b4b1f89",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f853dfee2327830e0e31e290872d101b",
     "grade": true,
     "grade_id": "train_regression_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run cell to test your implementation\n",
    "assert type(regr) == sklearn.linear_model.LinearRegression, \"Ensure that you train a Linear Regression Model\"\n",
    "assert hasattr(regr, \"coef_\"), \"Ensure that you fit the Linear Regression model to the data\"\n",
    "warnings.simplefilter(\"ignore\")\n",
    "assert regr.predict([[0.3]])[0] > 448, \"Ensure that you fit the model to the correct data\"\n",
    "assert test_error > 4245, \"Ensure that you calculate the Mean Squared Error correctly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cf98e8-5656-4aba-b974-2069ede6686f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run cell to visualize the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_train, Y_train, color=\"tab:orange\", alpha=0.1, label=\"train data\")\n",
    "ax.scatter(X_test, Y_test, color=\"tab:blue\", label=\"test data\")\n",
    "ax.plot(X_test, Y_test_pred, color=\"tab:green\", linewidth=3, label=\"predicted test data\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"s5\")\n",
    "_ = ax.set_ylabel(\"disease progression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea403f36",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2. Clustering\n",
    "For the clustering task, we will use an artificial dataset consisting of three different Gaussian distributions which serve as three clusters. The goal of this task is to assign a label to every data point that indicates to which of the clusters they belong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913fccc5-c61d-4cb4-a6a1-3a41b8cc10df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run cell to create and visualize the artificial dataset\n",
    "np.random.seed(0)\n",
    "gauss_1 = np.random.normal(loc=(10, 5), scale=(1, 3), size=(100, 2))\n",
    "gauss_2 = np.random.normal(loc=(12, 12), scale=(2, 1), size=(100, 2))\n",
    "gauss_3 = np.random.normal(loc=(16, 8), scale=1, size=(100, 2))\n",
    "data_cluster = np.concatenate([gauss_1, gauss_2, gauss_3])\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(data_cluster[:, 0], data_cluster[:, 1])\n",
    "ax.set_xlabel(\"feature 1\")\n",
    "ax.set_ylabel(\"feature 2\")\n",
    "_ = ax.set_title(\"original dataset without cluster labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec81799-b6e1-4cb3-aaef-d26eee5ac275",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 2.1 Use a K-Means Clustering algorithm to assign a cluster label to every data point in ``data_cluster``:\n",
    "* Create an instance of [sklearn.cluster.KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) called ``kmeans``. Set the parameters *n_clusters*: 3, *random_state*: 10, *n_init*: \"auto\".\n",
    "* Use ``kmeans`` methods to fit the model to ``data_cluster``\n",
    "* Store the predicted labels in a variable called ``labels_pred``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c285819-b81b-44f3-9794-e4538dbae233",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8df5a5ffa7bd0f5025eb2c6a43a87edf",
     "grade": false,
     "grade_id": "train_kmeans",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1335b8-68bc-40d2-bef4-2072b4dfa1ee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4e62db49b8353fed12e86b4e623b976",
     "grade": true,
     "grade_id": "train_kmeans_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run cell to test your implementation\n",
    "assert type(kmeans) == sklearn.cluster.KMeans, \"Ensure that you use a KMeans model for clustering\"\n",
    "assert kmeans.n_clusters == 3 and kmeans.random_state == 10 and kmeans.n_init == \"auto\", \"Check if you set the parameters of the KMeans model correctly\"\n",
    "assert hasattr(kmeans, \"cluster_centers_\"), \"Ensure that you fit the KMeans model to the data\"\n",
    "assert np.shape(labels_pred) == (300,), \"Ensure that you used the predicted labels of the trained KMeans model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3b3104-7fd5-48c4-befb-af22f4488a70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run cell to visualize the results\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(8, 4))\n",
    "axs[0].scatter(gauss_1[:, 0], gauss_1[:, 1], label=\"gauss_1\", alpha=0.5)\n",
    "axs[0].scatter(gauss_2[:, 0], gauss_2[:, 1], label=\"gauss_2\", alpha=0.5)\n",
    "axs[0].scatter(gauss_3[:, 0], gauss_3[:, 1], label=\"gauss_3\", alpha=0.5)\n",
    "axs[1].scatter(data_cluster[labels_pred==0, 0], data_cluster[labels_pred==0, 1], label=\"cluster_1\", alpha=0.5)\n",
    "axs[1].scatter(data_cluster[labels_pred==1, 0], data_cluster[labels_pred==1, 1], label=\"cluster_2\", alpha=0.5)\n",
    "_ = axs[1].scatter(data_cluster[labels_pred==2, 0], data_cluster[labels_pred==2, 1], label=\"cluster_3\", alpha=0.5)\n",
    "for ax, t in zip(axs, [\"original dataset\", \"KMeans clustered dataset\"]):\n",
    "    ax.legend()\n",
    "    ax.set_title(t)\n",
    "    ax.set_xlabel(\"feature 1\")\n",
    "    ax.set_ylabel(\"feature 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b89f49-c346-4286-a8cd-7f782b9ce0d9",
   "metadata": {},
   "source": [
    "#### 3. Classification\n",
    "For the classification task, we use the cortisol data set that has been included in some of the last assignments. Five Saliva features are computed for 26 participants which are divided into two groups *Intervention* and *Control*. The goal of this task is to fit a Random Forest Classifier to different (sub-)sets of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4e9e7c-136c-4a1b-9531-f5d682468c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run cell to load cortisol dataset\n",
    "cortisol = bp.example_data.get_saliva_example()\n",
    "sample_times = [-30, -1, 30, 40, 50, 60, 70]\n",
    "auc = bp.saliva.auc(\n",
    "    cortisol, saliva_type=\"cortisol\", sample_times=sample_times, compute_auc_post=True, remove_s0=True\n",
    ")\n",
    "max_inc = bp.saliva.max_increase(cortisol, saliva_type=\"cortisol\", remove_s0=True)\n",
    "slope = bp.saliva.slope(cortisol, sample_idx=[1, 4], sample_times=sample_times, saliva_type=\"cortisol\")\n",
    "\n",
    "cort_features = pd.concat([auc, max_inc, slope], axis=1)\n",
    "label_cortisol = np.array(cort_features.index.get_level_values(0))\n",
    "data_cortisol = cort_features.droplevel(0, axis=0)\n",
    "data_cortisol.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c5013e-0e40-4304-82f5-3c01de68a27a",
   "metadata": {},
   "source": [
    "##### 3.1 Train the classifier on the complete dataset:\n",
    "* Create an instance of [sklearn.ensemble.RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) called ``clf``. Set the parameters *n_estimators*: 100, *random_state*: 10.\n",
    "* Use the methods of ``clf`` to fit the model to ``data_cortisol`` and ``label_cortisol``\n",
    "* Calculate the mean accuracy of the dataset and save it in a variable ``data_score``\n",
    "* Describe your thoughts about the following questions in the cell below:\n",
    "    * How would you interpret this result? \n",
    "    * What can you tell about the generalizability of this model to unknown data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2256985-3b8c-4778-bd4f-a886940da790",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5486566-c42e-412c-b46b-f4920554638f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "371c0540b7ccd6ef71d3f52d46ef5091",
     "grade": false,
     "grade_id": "train_rf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(\"Classifier Data Score:\", data_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00740139-9922-4f43-9437-506644d0de72",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d11c8de99245118fd20f0fc89fdb834",
     "grade": true,
     "grade_id": "train_rf_test",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run cell to test your implementation\n",
    "assert type(clf) == sklearn.ensemble.RandomForestClassifier, \"Check if you train a classifer of type RandomForestClassifier\"\n",
    "assert clf.n_estimators == 100 and clf.random_state == 10, \"Check the parameters of the classifier\"\n",
    "assert hasattr(clf, \"classes_\"), \"Make sure that you fit the classifier\"\n",
    "warnings.simplefilter(\"ignore\")\n",
    "assert clf.predict([[300, 20, 40, 4, 0.03]])[0] == \"Control\" and clf.predict([[350, -10, 50, 0.7, 0]])[0] == \"Intervention\", \"Make sure to train the classifier on the correct data set\"\n",
    "assert data_score == 1, \"Make sure to train the classifier on the correct data set\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b53586e-4a6a-4b4d-853f-7d65e8ca831e",
   "metadata": {},
   "source": [
    "##### 3.2 Train the classifier on a train test split of the dataset:\n",
    "* Create an instance of [sklearn.ensemble.RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) called ``clf``. Set the parameters *n_estimators*: 100, *random_state*: 10.\n",
    "* Create a train test split of ``data_cortisol`` and ``label_cortisol`` using ``split_data()``. Use a test size of 0.3.\n",
    "* Use the methods of ``clf`` to fit the model to the training dataset\n",
    "* Calculate the mean accuracy of the training and test dataset separately and save it in variables called ``train_score`` and ``test_score``\n",
    "* Describe your thoughts about the following questions in the cell below:\n",
    "    * Why do you think is there such a big difference between that training and test model accuracy? \n",
    "    * What can you tell about the generalizability of this model to unknown data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7aaff8-c903-4b67-af72-5523f2b3def5",
   "metadata": {},
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1630a05-862b-430d-9310-4ef2bb638ea7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1062750916d0e0be8b9abd365a2b684a",
     "grade": false,
     "grade_id": "train_rf_split",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(\"Classifier Training Score:\", train_score)\n",
    "print(\"Classifier Test Score:\", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8d8999-06cb-4083-adb4-7b0b37561c6d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0b6fbdfdcc1681c132b6dab8a5f96fc",
     "grade": true,
     "grade_id": "train_rf_split_test",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run cell to test your implementation\n",
    "assert type(clf) == sklearn.ensemble.RandomForestClassifier, \"Check if you train a classifer of type RandomForestClassifier\"\n",
    "assert clf.n_estimators == 100 and clf.random_state == 10, \"Check the parameters of the classifier\"\n",
    "assert hasattr(clf, \"classes_\"), \"Make sure that you fit the classifier\"\n",
    "warnings.simplefilter(\"ignore\")\n",
    "assert clf.predict([[350, 20, 40, 4, 0.03]])[0] == \"Control\" and clf.predict([[350, -10, 50, 0.7, 0]])[0] == \"Intervention\", \"Make sure to train the classifier on the correct data set\"\n",
    "assert train_score == 1, \"Make sure to train the classifier on the correct data set\"\n",
    "assert test_score == 0.625, \"Make sure to calculate the accuracy for the separate test set\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a6afb3-725f-4216-84fb-0dec1a5e741a",
   "metadata": {},
   "source": [
    "##### 3.3 Train the classifier using Cross-Validation:\n",
    "* Create an instance of [sklearn.ensemble.RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) called ``clf``. Set the parameters *n_estimators*: 100, *random_state*: 10.\n",
    "* Use [sklearn.model_selection.cross_validate](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html) to train ``clf`` on the whole dataset using a 5-fold Cross-Validation. Set the parameter *cv*: 5\n",
    "* Calculate the mean standard deviation over the 5 test scores and save them in the variables called ``test_score_mean`` and ``test_score_std``\n",
    "* Describe you thoughts about the following questions in the cell below:\n",
    "    * What are the advantages of using Cross-Validation? \n",
    "    * What can you tell about the robustness of this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e74a4-49d7-4ec9-ae86-793b8c3fe5b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "*YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dd9023-fad9-4c5a-a11a-c1bfc38c9f7a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "017dc04fe9d4586d3a4efac0ff88f9e6",
     "grade": false,
     "grade_id": "train_rf_cv",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(\"Classifier Test score (mean \\u00B1 std): %.2f \\u00B1 %.2f\" % (test_score_mean, test_score_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d1e12-d190-4bbc-b6cd-9ad60a4193eb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddaed961bf1d6ee002be1ba43d6b0fe4",
     "grade": true,
     "grade_id": "train_rf_cv_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run cell to test your implementation\n",
    "assert type(clf) == sklearn.ensemble.RandomForestClassifier, \"Check if you train a classifer of type RandomForestClassifier\"\n",
    "assert clf.n_estimators == 100 and clf.random_state == 10, \"Check the parameters of the classifier\"\n",
    "assert hasattr(clf, \"classes_\") == False, \"Make sure to train the classifiers using cross_validate\"\n",
    "assert test_score_mean > 0.6 and test_score_std > 0.11, \"Make sure to calculate the mean accuracy and standard deviation from the return values from cross_validate()\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ads_exercise",
   "language": "python",
   "name": "ads_exercise"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
